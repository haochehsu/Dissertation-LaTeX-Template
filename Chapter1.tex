Equation \ref{myEquation} or \cref{myEquation} or Equation \eqref{myEquation}. \lipsum[105-106] More are discussed in \Cref{discussion}. This chapter explores the aspect of firm competitions. \lipsum[11] \lipsum[11]Download this template at the \href{https://github.com/howardhsumail/Dissertation-LaTeX-Template.git}{Github repository}.

\section{Introduction}

The {\color{mycolor}competition} can be illustrated with the following graph with the implementation is presented in Listing \ref{mypythoncode} or \cref{mypythoncode}.

\begin{lemma}[This is a lemma]\label{lma1}
  \onehalfspacing
  \begin{enumerate}[(a)]
    \setlength{\itemsep}{-0.5pt}
    \item[] \hfill
    \item For any feasible disclosure policy  $G\in \mathcal{G}$, $W_G(p)$ is a convex function. Moreover, for all $p\in[0,1]$, $W_{G_{\underline{\pi}}}(p)\leq W_G(p)\leq W_{G_{\overline{\pi}}(p)}$.
    \item The converse of the above statement is also true. That is, if $W:[0,1]\to \mathbbm{R}$ is a convex function that satisfies $W_{G_{\underline{\pi}}}(p)\leq W(p)\leq W_{G_{\overline{\pi}}(p)}$, then there exists a feasible $G\in\mathcal{G}$ such that $W_G(p) = W(p)$ for all $p\in[0,1]$.
  \end{enumerate}
\end{lemma}

\lipsum[14]

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{Graph/pic.pdf}
  \caption{This is a graph}
  \hlabel{picture1}
  \hspace*{-0.6cm}
  \begin{minipage}{1.04\textwidth}
    \onehalfspacing
    \vspace*{0.12cm}
    \begin{tablenotes}
      \footnotesize
      \item\textit{Note:} some notes. The graph should be self-contained. \lipsum[66]
    \end{tablenotes}
  \end{minipage}
\end{figure}

\section{Model}
\lipsum[3-4] The proof is discussed in Appendix \ref{ch1_proof} or \cref{ch1_proof}.

\begin{theorem}[Envelope Theorem]
\onehalfspacing
  Only the direct effects of a change in an exogenous variable need be considered, even though the exogenous variable may enter the maximum value function indirectly as part of the solution to the endogenous choice variables.
\end{theorem}

\section{Comparative Statics}
This is also demonstrated in Figure \ref{picture1} or \cref{picture1}.

\begin{lstlisting}[style=python_code, caption={Long short-term memory}, label=mypythoncode]
class network_LSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=256, output_size=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size)

        # fully-connected
        self.linear = nn.Linear(hidden_size, output_size)

        self.hidden = (
            torch.zeros(1, 1, self.hidden_size),
            torch.zeros(1, 1, self.hidden_size)
        )

    def forward(self,vec):
        lstm_output, self.hidden = self.lstm(vec.view(len(vec),1,-1), self.hidden)
        prediction = self.linear(lstm_output.view(len(vec),-1))
        return prediction[-1]
\end{lstlisting}

\section{Conclusion}
\lipsum[7-9]